# VizPrompt 開発計画書

## 1. プロジェクト概要

VizPromptは、チャット履歴をフローとして視覚的に管理するノードベースのインターフェースです。会話の分岐を可視化し、履歴の動的な再構成を可能にするツールとなります。

## 2. 開発フェーズ

開発は以下の5つのフェーズに分けて進めていきます。各フェーズでは特定の機能セットを完成させ、段階的に機能を拡充していきます。

### フェーズ1: 基盤構築と基本データ構造（現状反映版）

**目標**: コアとなるノードデータ構造とファイル操作システムの初期実装

#### 実施内容・進捗:
1. プロジェクト初期設定・ディレクトリ構成
   - 基本設定ファイル・ディレクトリ作成（完了）
   - 開発環境（Python/pyproject.toml/README）整備（完了）

2. データモデル・ファイル操作
   - ノードXML形式の設計・保存/読み込み機能の実装（`vizprompt/core/node.py`）（完了）
   - ノードマップ（TSV）管理機能の実装（`vizprompt/core/node.py`）（完了）
   - メタデータYAML形式の実装（未着手）
   - フォルダ管理システム（自動生成のみ実装、詳細管理は未着手）

3. コアモジュール
   - NodeSaverクラスの実装（完了）
   - Flow, Metadata, Storageクラスの実装（未着手）
   - オブジェクトシリアライズ・デシリアライズ機能（未着手）

4. テスト
   - 単体テスト（未着手）

※ Gemini API連携・CLIコマンド等はフェーズ2以降で本格対応予定

### フェーズ2: LLM連携とコンテキスト管理

**目標**: 各種LLMとの連携機能とコンテキスト管理の実装

#### タスク:
1. LLM基本インターフェース設計
   - 共通APIインターフェース定義
   - トークンカウントユーティリティ

2. OpenAI互換コネクタ実装
   - APIクライアント実装
   - エラーハンドリング

3. Geminiコネクタ実装
   - Gemini API連携
   - レスポンス処理

4. Ollamaコネクタ実装
   - ローカルLLM連携
   - 設定管理

5. 要約・タグ付け機能の実装
   - LLMを用いた要約生成
   - 自動タグ付け機能
   - メタデータ更新処理

6. コンテキスト構築機能の実装
   - 参照ノードの取得と整形
   - 効率的なコンテキスト組み立て

### フェーズ3: 会話フロー管理とCLI

**目標**: 会話フローの基本機能とコマンドラインインターフェースの実装

#### タスク:
1. 基本フロー管理機能実装
   - 線形会話フロー管理
   - ノード間接続処理
   - フロー定義ファイル操作

2. CLIフレームワーク実装
   - コマンド引数処理
   - サブコマンド構造設計

3. 基本コマンド実装
   - ノード作成・取得・更新コマンド
   - フロー操作コマンド
   - 検索・フィルタリングコマンド

4. サーバーモード実装
   - WebSocketサーバー基本機能
   - イベント発行システム

5. テストとドキュメンテーション
   - 統合テスト作成
   - CLI使用ドキュメント作成

### フェーズ4: 分岐機能とWebSocket API

**目標**: 会話の分岐機能の完全実装とWebSocket APIの整備

#### タスク:
1. ノード分岐機能拡張
   - 分岐作成・管理機能
   - ノード接続変更操作
   - 分岐の視覚化データ準備

2. WebSocket API実装
   - メッセージプロトコル実装
   - アクションハンドラー作成
   - 非同期イベント通知

3. API機能の拡充
   - 詳細なノード操作API
   - フロー管理API
   - 検索・フィルタリングAPI

4. イベントシステム実装
   - イベントタイプ定義
   - 購読・通知機構
   - リアルタイム更新

5. セキュリティとエラーハンドリング
   - 入力バリデーション
   - エラー応答フォーマット
   - 例外処理の統一

### フェーズ5: フロントエンドとインテグレーション

**目標**: Webフロントエンドの実装とシステム統合

#### タスク:
1. フロントエンド基盤構築
   - HTML/CSS基本構造
   - WebSocket接続管理
   - イベントリスナー設定

2. グラフ可視化実装
   - D3.js/Cytoscape.js連携
   - ノードとエッジの描画
   - インタラクション処理

3. UI要素実装
   - プロンプト・応答表示
   - タグ・メタデータ表示
   - ユーザー入力フォーム

4. 操作機能実装
   - ノード作成・編集
   - 接続管理UI
   - 検索・フィルタリングUI

5. システム統合テスト
   - エンドツーエンドテスト
   - パフォーマンステスト

6. 最終ドキュメンテーションと公開準備
   - 使用マニュアル作成
   - API仕様書作成
   - GitHubリポジトリ準備

## 3. マイルストーン

| マイルストーン | 予定日 | 成果物 |
|--------------|-------|-------|
| 基盤構築完了 | 6月6日 | コアデータ構造とファイルシステム操作が機能するベースシステム |
| LLM連携完了 | 6月27日 | 複数LLMプロバイダー対応と要約・タグ付け機能が動作するシステム |
| CLI機能完了 | 7月18日 | コマンドラインから全基本機能が操作可能なバージョン |
| 分岐機能とAPI完了 | 8月8日 | 分岐機能とWebSocket APIが完全に動作するバージョン |
| v1.0リリース | 9月5日 | フロントエンド完成・全機能統合されたリリースバージョン |

## 4. 技術スタック

### バックエンド
- 言語: Python 3.10+
- ファイル形式: XML, YAML, TSV
- WebSocket: websockets, asyncio
- ユーティリティ: tiktoken (トークンカウント)

### フロントエンド
- 言語: HTML5, CSS3, JavaScript (ES6+)
- グラフ可視化: D3.js または Cytoscape.js
- 非依存: フレームワークに依存しない設計

### LLM連携
- OpenAI互換API
- Google Gemini API
- Ollama (ローカルLLM)

## 5. リスクと対応策

| リスク | 影響 | 対応策 |
|------|------|------|
| LLM API仕様変更 | LLM連携機能の停止 | コネクタのモジュール化と抽象化により迅速な対応を可能に |
| パフォーマンス問題 | 大規模データでの動作遅延 | キャッシュ戦略と段階的データロード、インデックス最適化 |
| 複雑なノード関係のUI表現 | ユーザー理解の困難さ | 段階的な情報表示と視覚化の最適化、フィルタリング機能 |
| トークン使用量の最適化 | コスト増加と応答遅延 | 要約の効果的利用とコンテキスト圧縮戦略の実装 |
| 互換性の問題 | 異なるLLMでの一貫しない結果 | プロバイダー固有の調整とフォールバック戦略 |

## 6. 開発・テスト・デプロイメント戦略

### 開発戦略
- モジュール単位での開発と統合
- テスト駆動開発の採用
- GitHubでのバージョン管理とプルリクエスト方式

### テスト戦略
- ユニットテスト: pytest
- 統合テスト: シナリオベースのテスト
- モック: LLM呼び出しのモック化
- カバレッジ: 80%以上のコードカバレッジを目標

### デプロイメント
- PyPI公開によるライブラリとしてのインストール
- シングルバイナリビルドオプション
- Docker化によるサーバー環境での運用サポート

## 7. メンテナンス計画

### バージョニング
- セマンティックバージョニングの採用
- 後方互換性の維持を重視

### 拡張予定機能
1. カスタムLLMプロバイダープラグイン
2. データベースバックエンドオプション
3. 高度な分析とビジュアライゼーション
4. チームコラボレーション機能

### サポート計画
- GitHubイシューによるバグ報告と対応
- 定期的なメンテナンスリリース (セキュリティと依存関係更新)
- コミュニティフィードバックによる継続的改良
